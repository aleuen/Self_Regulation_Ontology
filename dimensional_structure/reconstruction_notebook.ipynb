{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstructing out-of-sample DVs\n",
    "\n",
    "Given a quantitative ontology, or psychological space, that DVs can be projected into, how can we deterine the embedding of new variables?\n",
    "\n",
    "Currently, our embedding is determined by factor analysis. Thus ontological embedding are only known for the DVs entered into the original model. How could we extend this?\n",
    "\n",
    "One possibility is measuring new variables in the same population that completed our original battery. After doing this we could either (1) run the model anew, or (2) use linear regression to map the already discovered factors onto the new variables. The former is better, but results in small changes to the actual factors with each new variable. The latter method ensures that our factors stay the same. Neither is scalable, however, as we do not, in general, have access to a constant population that can be remeasured whenever new measures come into the picture.\n",
    "\n",
    "Another possibility that works with new populations requires that the new population completes the entire battery used to estimate the original factors, in addition to whatever new variables are of interest. Doing so allows the calculation of factor scores for this new population based on the original model, which can then be mapped to the new measures of interest. This allows researchers to capitalize on the original model (presumably fit on more subjects than the new study), while expanding the ontology. Problems exist here, however.\n",
    "- The most obvious problem is that you have to measure the new sample on the entire battery used to fit the original EFA model. Given that this takes many hours (the exact number depending on whether tasks, surveys or both are used), this is exceedingly impractical. In our cas we did have our new fMRI sample take the entire battery (or at least a subset of participants), so this problem isn't as relevant\n",
    "- Still problems remain. If N is small, the estimate of the ontological embedding for new DVs is likely unstable.\n",
    "\n",
    "This latter problem necessitates some quantitative exploration. This notebook simulates the issue by:\n",
    "1. Removing a DV from the original ontology dataset\n",
    "2. Performing EFA on this subset\n",
    "3. Using linear regression to map these EFA factors to the left out variable\n",
    "\n",
    "(3) is performed on smaller population sizes to reflect the reality of most studies (including ours) and is repeated to get a sense of the mapping's variability\n",
    "\n",
    "### Small issues not currently addressed\n",
    "\n",
    "- The EFA model is fit on the entire population. An even more stringent simulation would subset the subjects used in the \"new study\" and fit the EFA model on a completely independent group. I tried this once - the factor scores hardly differed. In addition, I want the EFA model to be as well-powered as possible, as that will be the reality for this method moving forward\n",
    "- I am currently not holding out entire tasks, but only specific DVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dimensional_structure.prediction_utils import assess_var_reconstruction\n",
    "from selfregulation.utils.result_utils import load_results\n",
    "from selfregulation.utils.utils import get_recent_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results(get_recent_dataset())['task']\n",
    "c = results.EFA.results['num_factors']\n",
    "n_reps = 100\n",
    "n_vars=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run simulation for every variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_results = {}\n",
    "pop_sizes = [50, 100, 200, 400]\n",
    "var_list = np.random.choice(results.data.columns, n_vars, replace=False)\n",
    "for pop_size in pop_sizes:     \n",
    "    var_out = {}\n",
    "    for var in var_list:\n",
    "        var_out[var] = assess_var_reconstruction(results, var,\n",
    "                                                 pseudo_pop_size=pop_size,\n",
    "                                                 n_reps=n_reps)\n",
    "    reconstruction_results[pop_size] = var_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_df = pd.DataFrame()\n",
    "for pop_size, out in reconstruction_results.items():\n",
    "    for k, v in out.items():\n",
    "        reps = v[1].shape[0]\n",
    "        combined = pd.concat([v[0], v[2], v[1].T], axis=1).T\n",
    "        combined.reset_index(drop=True, inplace=True)\n",
    "        combined.insert(combined.shape[1], \n",
    "                        'score-corr', \n",
    "                        combined.T.corr().iloc[1:,0])\n",
    "        combined.insert(combined.shape[1],\n",
    "                       'score-MSE',\n",
    "                       ((v[1]-v[0])**2).mean(1))\n",
    "        label = ['true'] + ['full_reconstruct'] + ['partial_reconstruct'] * reps\n",
    "        combined.loc[:, 'label'] = label\n",
    "        combined.loc[:, 'var'] = k\n",
    "        combined.loc[:, 'pop_size'] = pop_size\n",
    "        reconstruction_df = pd.concat([reconstruction_df, combined])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_df.query('label==\"partial_reconstruct\"') \\\n",
    "    .groupby(['pop_size','var']).agg([np.mean, np.std])[['score-MSE', 'score-corr']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Of concern is the average correspondence and variability between the estimated ontological fingerprint of a DV and its \"ground-truth\" (the original estimate when it was part of the EFA model)\n",
    "\n",
    "One way to look at this is just the average reconstruction score (e.g., for example) and variability of reconstruction score as a function of pseudo-pop-size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import MDS, TSNE\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = reconstruction_df.query('label==\"partial_reconstruct\"') \\\n",
    "            .groupby(['pop_size','var']) \\\n",
    "            .agg([np.mean, np.std])[['score-corr']].reset_index()\n",
    "plot_df.columns = [' '.join(col).strip() for col in plot_df.columns.values] # flatten hierarchical columns\n",
    "plot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.swarmplot(x='pop_size', y='score-corr mean', data=plot_df, size=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complicate, we can visualize this by looking at the MDS plotting:\n",
    "1. The original DVs\n",
    "2. The \"best\" reconstruction using all the data\n",
    "3. The n_reps simulated estimates with a smaller population size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_df.sort_values(by='label', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDS\n",
    "mds_reduced = []\n",
    "tsne_reduced = []\n",
    "for pop_size in pop_sizes:\n",
    "    mds = MDS(2, dissimilarity='precomputed')\n",
    "    tsne = TSNE(2, metric='precomputed')\n",
    "    subset = reconstruction_df.query('pop_size == %s'% pop_size)\n",
    "    reconstructions = subset.iloc[:, :c]\n",
    "    distances = squareform(pdist(reconstructions, metric='correlation'))\n",
    "    #mds_reduced.append(mds.fit_transform(distances)) # taking too long\n",
    "    tsne_reduced.append(tsne.fit_transform(1-reconstructions.T.corr()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_subset = reconstruction_df.query('pop_size == %s'% pop_sizes[-1])\n",
    "base_colors = sns.color_palette(n_colors=len(var_list))\n",
    "color_map = {var_list[i]:base_colors[i] for i in range(len(var_list))}\n",
    "color_list = list(tmp_subset.loc[:,'var'].apply(lambda x: color_map[x]))\n",
    "edge_colors = [color_list[i] if x=='partial_reconstruct' else [1,1,1] for i,x in enumerate(tmp_subset.label)]\n",
    "size_list = [30 if x=='partial_reconstruct' else 200 for x in tmp_subset.label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_pop = len(pop_sizes)\n",
    "f,axes = plt.subplots(N_pop,1,figsize=(8,8*N_pop))\n",
    "for ax, reduced, pop_size in zip(axes, tsne_reduced, pop_sizes):\n",
    "    ax.scatter(reduced[:,0], reduced[:,1], c=color_list, s=size_list, edgecolors=edge_colors, linewidth=2)\n",
    "    ax.set_title('Pseudo-Population Size: %s' % pop_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
