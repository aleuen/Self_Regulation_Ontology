{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import igraph\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from selfregulation.utils.utils import get_behav_data\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from selfregulation.utils.r_to_py_utils import GPArotation, missForest, psychFA\n",
    "from fancyimpute import SoftImpute\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate best number of components based on CV ML (SKlearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Load both cleaned and imputed data. Set up a helper function to quickly impute data within the Kfold split. Can't use data where imputation was performed over the whole dataset as there will be data leakage between training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using for speed, for the time being\n",
    "def SoftImpute_df(data):\n",
    "    imputed_mat = SoftImpute(verbose=False).complete(data)\n",
    "    return pd.DataFrame(data=imputed_mat, columns=data.columns, index=data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "data = get_behav_data(file = 'taskdata_clean.csv', full_dataset = True)\n",
    "# imputed data\n",
    "imputed_data = get_behav_data(file = 'meaningful_variables_imputed.csv')\n",
    "imputed_task_data = get_behav_data(file = 'taskdata_imputed.csv')\n",
    "imputed_survey_data = imputed_data.drop(imputed_task_data.columns, axis = 1)\n",
    "imputed_datasets = {'survey': imputed_survey_data, 'task': imputed_task_data, 'complete': imputed_data}\n",
    "# cleaned data \n",
    "cleaned_data = get_behav_data( file = 'meaningful_variables_clean.csv')\n",
    "cleaned_task_data = get_behav_data( file = 'taskdata_clean.csv')\n",
    "cleaned_survey_data = cleaned_data.drop(cleaned_task_data.columns, axis = 1)\n",
    "cleaned_datasets = {'survey': cleaned_survey_data, 'task': cleaned_task_data, 'complete': cleaned_data}\n",
    "\n",
    "results = {'survey': {}, 'task': {}, 'complete': {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine optimal number of factors\n",
    "Two methods\n",
    "\n",
    "Sklearn using cross-validated prediction\n",
    "R's Psych package looking for best fitting model across all the data, penalized for complexity (BIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def CV_optimal_components(data, cv):\n",
    "    # find best number of components\n",
    "    n_components = range(1,12)\n",
    "    best_c = 0\n",
    "    component_scores = []\n",
    "    for c in n_components:\n",
    "        fa=FactorAnalysis(c)\n",
    "        scores = []\n",
    "        # cross-validate fit across different train/test splits\n",
    "        for train_index, test_index in kf.split(data.values):\n",
    "            data_train, data_test = data.iloc[train_index], \\\n",
    "                                    data.iloc[test_index]\n",
    "            # Impute (replace with missForest later)\n",
    "            imputed_train = SoftImpute_df(data_train)\n",
    "            imputed_test = SoftImpute_df(data_test)\n",
    "            # fit model on training\n",
    "            fa.fit(imputed_train)\n",
    "            # score on test set\n",
    "            scores.append(fa.score(imputed_test))\n",
    "        #average score across kfolds\n",
    "        score = np.mean(scores)\n",
    "        component_scores.append(score)\n",
    "    best_c = np.argmax(component_scores)\n",
    "    print('Best Component: ', best_c)\n",
    "    return best_c, component_scores\n",
    "\n",
    "kf = KFold(n_splits = 4)\n",
    "for label,data in cleaned_datasets.items():\n",
    "    print('Dataset: %s' % label)\n",
    "    best_c, scores= CV_optimal_components(data, kf)\n",
    "    results[label]['sklearn_best_c'] = best_c\n",
    "    results[label]['CV_scores'] = scores\n",
    "    \n",
    "for score in [d['CV_scores'] for d in results.values()]:\n",
    "    plt.plot(range(len(score)),score, '-o')\n",
    "plt.legend(results.keys(), loc = 'best')\n",
    "plt.xlabel('# Components')\n",
    "plt.ylabel('Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate best number of components using BIC (R Psych package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def BIC_optimal_components(data):\n",
    "    BICs = []\n",
    "    outputs = []\n",
    "    n_components = range(1,12)\n",
    "    for c in n_components:\n",
    "        fa, output = psychFA(data, c)\n",
    "        BICs.append(output['BIC'])\n",
    "        outputs.append(output)\n",
    "    best_c = np.argmin(BICs)\n",
    "    print('Best Component: ', best_c)\n",
    "    return best_c, BICs\n",
    "\n",
    "for label,data in imputed_datasets.items():\n",
    "    print('Dataset: %s' % label)\n",
    "    best_c, BICs = BIC_optimal_components(data)\n",
    "    results[label]['psych_best_c'] = best_c\n",
    "    results[label]['psych_BICs'] = BICs\n",
    "    \n",
    "for bic in [d['psych_BICs'] for d in results.values()]:\n",
    "    plt.plot(range(len(bic)),bic, '-o')\n",
    "plt.legend(results.keys(), loc = 'best')\n",
    "plt.xlabel('# Components')\n",
    "plt.ylabel('Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "results['complete']['psych_best_c']": "9",
     "results['complete']['sklearn_best_c']": "8",
     "results['survey']['psych_best_c']": "10",
     "results['survey']['sklearn_best_c']": "10",
     "results['task']['psych_best_c']": "6",
     "results['task']['sklearn_best_c']": "7"
    }
   },
   "source": [
    "# Interpret Task Factor Analysis Solution\n",
    "Above we found that the best number of components were as follows:\n",
    "\n",
    "**SKlearn**:\n",
    "\n",
    "Task: {{results['task']['sklearn_best_c']}}\n",
    "\n",
    "Survey: {{results['survey']['sklearn_best_c']}}\n",
    "\n",
    "Complete: {{results['complete']['sklearn_best_c']}}\n",
    "\n",
    "**Psych**:\n",
    "\n",
    "Task: {{results['task']['psych_best_c']}}\n",
    "\n",
    "Survey: {{results['survey']['psych_best_c']}}\n",
    "\n",
    "Complete: {{results['complete']['psych_best_c']}}\n",
    "\n",
    "Because psych performs rotation, is more widely used, we will use that as our main analysis. All analyses will be repeated with the sklearn pipeline as well. First we will look at the best number of components, then we will look at the solution at each dimensionality and plot the relationships between dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret best components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_results = results['task']\n",
    "best_c = task_results['psych_best_c']\n",
    "data = imputed_datasets['task']\n",
    "fa, output = psychFA(data, best_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Print top variables for each factor\n",
    "def print_top_factors(fa_output, n = 4):\n",
    "    # number of variables to display\n",
    "    loading_df = pd.DataFrame(fa_output['loadings'], index=data.columns)\n",
    "    for i,column in loading_df.iteritems():\n",
    "        sort_index = np.argsort(abs(column))[::-1] # descending order\n",
    "        top_vars = column[sort_index][0:n]\n",
    "        print('\\nFACTOR %s' % i)\n",
    "        print(top_vars)\n",
    "    return loading_df\n",
    "        \n",
    "loading_df = print_top_factors(output, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of factor loadings and plot of histogram relationship\n",
    "We can see that factors 4 and 5 are really just pulling from a few variables. They are really \"task\" factors - tapping discounting tasks and the ART task respectively. The CCT Hot \"number of cards chosen\" maps onto factor 5, but to a far smaller extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "loading_df.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factors are roughly orthogonal. Orthogonality wasn't enforced, and factors were rotated using \"oblimin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(loading_df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation/Naming of Factors \n",
    "\n",
    "Factor 0: Speed of Processing (Drift)\n",
    "\n",
    "Factor 1: Memory/Abstraction\n",
    "\n",
    "Factor 2: Stimulus Encoding Time (Non-Decision)\n",
    "\n",
    "Factor 3: Response Caution (Threshold)\n",
    "\n",
    "Factor 4: Dicount Rate\n",
    "\n",
    "Factor 5: Risk Taking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the same for all other dimensionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "factor_names = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One Factor**\n",
    "\n",
    "Factor 0: Cognitive Ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fa, output = psychFA(data, 1)\n",
    "loading_df = print_top_factors(output)\n",
    "factor_names[1] = ['CogAblty']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two Factors**\n",
    "\n",
    "Factor 0: Speed of Processing (Drift)\n",
    "\n",
    "Factor 1: Discount Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fa, output = psychFA(data, 2)\n",
    "loading_df = print_top_factors(output, 6)\n",
    "factor_names[2] = ['Drift','Discount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Three Factors**\n",
    "\n",
    "Factor 0: Speed of Processing (Drift)\n",
    "\n",
    "Factor 1: Encoding Time (Non-Decision)\n",
    "\n",
    "Factor 2: Discount Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fa, output = psychFA(data, 3)\n",
    "loading_df = print_top_factors(output, 6)\n",
    "factor_names[3] = ['Drift','Non-Dec','Discount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Four Factors**\n",
    "\n",
    "Factor 0: Speed of Processing (Drift)\n",
    "\n",
    "Factor 1: Memory/Abstraction\n",
    "\n",
    "Factor 2: Encoding Time (Non-Decision)\n",
    "\n",
    "Factor 3: Discount Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fa, output = psychFA(data, 4)\n",
    "loading_df = print_top_factors(output, 6)\n",
    "factor_names[4] = ['Drift','Mem','Non-Dec','Discount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Five Factors**\n",
    "\n",
    "Factor 0: Speed of Processing (Drfit)\n",
    "\n",
    "Factor 1: Memory/Abstraction \n",
    "\n",
    "Factor 2: Encoding Time (Non-Decision)\n",
    "\n",
    "Factor 3: Response Caution (Threshold)\n",
    "\n",
    "Factor 4: Discount Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fa, output = psychFA(data, 5)\n",
    "loading_df = print_top_factors(output, 6)\n",
    "factor_names[5] = ['Drift','Mem','Non-Dec','Thresh','Discount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Six Factors**\n",
    "\n",
    "Factor 0: Speed of Processing (Drift)\n",
    "\n",
    "Factor 1: Memory/Abstraction\n",
    "\n",
    "Factor 2: Stimulus Encoding Time (Non-Decision)\n",
    "\n",
    "Factor 3: Response Caution (Threshold)\n",
    "\n",
    "Factor 4: Dicount Rate\n",
    "\n",
    "Factor 5: Risk Taking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fa, output = psychFA(data, 6)\n",
    "loading_df = print_top_factors(output)\n",
    "factor_names[6] = ['Drift','Mem','Non-Dec','Thresh','Discount','Risk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seven Factors**\n",
    "\n",
    "Factor 0: Speed of Processing (Drift)\n",
    "\n",
    "Factor 1: Information Use\n",
    "\n",
    "Factor 2: Stimulus Encoding Time (Non-Decision)\n",
    "\n",
    "Factor 3: Dicount Rate\n",
    "\n",
    "Factor 4: Response Caution (Threshold)\n",
    "\n",
    "Factor 5: Response Caution (Speed of action)\n",
    "\n",
    "Factor 6: Risk Taking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fa, output = psychFA(data, 7)\n",
    "loading_df = print_top_factors(output, 8)\n",
    "factor_names[7] = ['Drift','Inf-Use','Non-Dec','Discount','Thresh','Speed', 'Risk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eight Factors**\n",
    "\n",
    "Factor 0: Speed of Processing (Drift)\n",
    "\n",
    "Factor 1: Memory/Abstraction\n",
    "\n",
    "Factor 2: Stimulus Encoding Time (Non-Decision)\n",
    "\n",
    "Factor 3: Dicount Rate\n",
    "\n",
    "Factor 4: Response Caution (Threshold)\n",
    "\n",
    "Factor 5: Risk Taking\n",
    "\n",
    "Factor 6: Information Use (CCT information)\n",
    "\n",
    "Factor 7: Response Caution (Speed of action on IST and TOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fa, output = psychFA(data, 8)\n",
    "loading_df = print_top_factors(output, 6)\n",
    "factor_names[8] = ['Drift','Mem','Non-Dec','Discount','Thresh','Risk','Inf-Use','Speed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot relationships between solutions at different dimensionalities\n",
    "\n",
    "Each factor analytic solution gives us some components. By correlating the components at one level with the next, we can get a sense for how similar these components are. When we move to 3 dimensions from 2, is it the case that one of the dimensions just split? Or is there a new, non-hierarchical parcellation of the space?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper functions for plotting dimensionality relations\n",
    "# compute FA with different dimensionalities and quantify similarity\n",
    "def dimensional_similarity(factors, reference):\n",
    "    ''' This function returns an N x M correlation matrix where N is the number\n",
    "    of reference (lower dimension) factors and M is the number of higher dimension\n",
    "    factors\n",
    "    '''\n",
    "    relation = np.corrcoef(reference.T, factors.T)\n",
    "    # each row is a reference factor, each column a new factor\n",
    "    relation=relation[:reference.shape[1], reference.shape[1]:]\n",
    "    return relation\n",
    "\n",
    "def construct_relational_tree(similarities, labels=None, filey=None):\n",
    "    ''' Takes a list of similarities and constructs a tree graph, then plots\n",
    "    '''\n",
    "    G = igraph.Graph()\n",
    "    layer_start = 0\n",
    "    colors = ['red','blue','green','violet']*4\n",
    "    for similarity in similarities:\n",
    "        curr_color = colors.pop()\n",
    "        origin_length = similarity.shape[0]\n",
    "        target_length = similarity.shape[1]\n",
    "        if len(G.vs)==0:\n",
    "            G.add_vertices(origin_length)\n",
    "        G.add_vertices(target_length)\n",
    "        for i in range(origin_length):\n",
    "            for j in range(target_length):\n",
    "                G.add_edge(i+layer_start,j+origin_length+layer_start,\n",
    "                           weight=abs(similarity[i,j]*5),color = curr_color)\n",
    "        layer_start+=similarity.shape[0]\n",
    "    layout = G.layout_reingold_tilford(root = [0])\n",
    "    if labels == None:\n",
    "        labels = G.vs.indices\n",
    "    plot = igraph.plot(G, layout=layout, **{'inline': True, 'vertex_label': labels, \n",
    "                                            'vertex_color': 'white',\n",
    "                                            'vertex_size': 50, \n",
    "                                         'edge_width':[w for w in G.es['weight']], \n",
    "                                         'edge_color': G.es['color'], \n",
    "                                            'bbox': (1000,1000), 'margin':100})\n",
    "    if filey:\n",
    "        plot.save(filey)\n",
    "    #G.write_dot('test.dot')\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct array of component similarities\n",
    "similarities = []\n",
    "components = np.sort(list(factor_names.keys()))\n",
    "reference = None\n",
    "for c in components:\n",
    "    fa, output = psychFA(data,c)\n",
    "    result = pd.DataFrame(output['scores'], data.index)\n",
    "    if reference is not None:\n",
    "        similarity = dimensional_similarity(result,reference)\n",
    "        similarities.append(similarity)\n",
    "    reference = result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot component similarity tree\n",
    "labels = []\n",
    "for c in components:\n",
    "    labels+=factor_names[c]\n",
    "similarity_tree = construct_relational_tree(similarities, filey=path.join('Plots', 'task_graph.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for label,data in imputed_datasets.items():\n",
    "    best_c = task_data['sklearn_best_c']\n",
    "    # construct array of component similarities\n",
    "    similarities = []\n",
    "    components = range(1,best_c+2)\n",
    "    reference = None\n",
    "    for c in components:\n",
    "        fa=FactorAnalysis(c)\n",
    "        result = pd.DataFrame(fa.fit_transform(data.values), data.index)\n",
    "        if c>1:\n",
    "            result = GPArotation(result, method='oblimin')\n",
    "        if reference is not None:\n",
    "            similarity = dimensional_similarity(result,reference)\n",
    "            similarities.append(similarity)\n",
    "        reference = result\n",
    "\n",
    "    # plot tree\n",
    "    labels = []\n",
    "    for c in np.sort(list(factor_names.keys())):\n",
    "        labels+=factor_names[c]\n",
    "    similarity_tree = construct_relational_tree(similarities, filey=path.join('Plots', + label +'_graph.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Take max for each dimension?\n",
    "binarized_similarities = []\n",
    "for s in similarities:\n",
    "    r,c = s.shape\n",
    "    binarized_similarities.append(s==np.resize(s.max(axis=1),(c,r)).T)\n",
    "\"\"\"\n",
    "thresh = .5\n",
    "binarized_similarities = []\n",
    "for s in similarities:\n",
    "    binarized_similarities.append((s>thresh)*1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of optimal factor structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum over task variables (extrememly questionable) and then plot the relationships across tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot_utils import dendroheatmap_left\n",
    "# *****************************************************************************\n",
    "# sum over tasks\n",
    "# *****************************************************************************\n",
    "tasks = np.unique([i.split('.')[0] for i in data.columns])\n",
    "task_sums = {}\n",
    "for task in tasks:  \n",
    "    task_sums[task] = result.filter(regex=task,axis=0).sum()\n",
    "task_sums = pd.DataFrame(task_sums).T\n",
    "dendroheatmap = dendroheatmap_left(task_sums.T.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multidimensional scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# visualize the similarity of the measurements in FA space\n",
    "# *****************************************************************************\n",
    "\n",
    "from data_preparation_utils import convert_var_names\n",
    "from graph_utils import distcorr_mat\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "import seaborn as sns\n",
    "\n",
    "seed = np.random.RandomState(seed=3)\n",
    "mds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=seed,\n",
    "                   dissimilarity=\"precomputed\", n_jobs=1)\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, random_state=seed, metric=\"precomputed\")\n",
    "\n",
    "# compute distances between variables\n",
    "# using distance correlation\n",
    "# result_distances = 1-distcorr_mat(result.T.values)\n",
    "# euclidean\n",
    "result_distances = euclidean_distances(result)\n",
    "# transform\n",
    "mds_transform = mds.fit_transform(result_distances)\n",
    "tsne_transform = tsne.fit_transform(result_distances)\n",
    "\n",
    "# plot\n",
    "tasks = [i.split('.')[0] for i in result.index]\n",
    "colors = sns.color_palette(\"husl\", len(np.unique(tasks)))\n",
    "\n",
    "fig, ax = sns.plt.subplots(figsize = (20,20))\n",
    "ax.scatter(mds_transform[:,0], mds_transform[:,1])\n",
    "\n",
    "variables = convert_var_names(list(result.index))\n",
    "for i, txt in enumerate(variables):\n",
    "    ax.annotate(txt, (mds_transform[i,0],mds_transform[i,1]), size = 15)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py34]",
   "language": "python",
   "name": "conda-env-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
